{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../assets/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"Date\"] = pd.to_datetime(train[\"Date\"], infer_datetime_format=True)\n",
    "train['Address'] = train['Address'].astype('category')\n",
    "train['Species'] = train['Species'].astype('category')\n",
    "train['Street'] = train['Street'].astype('category')\n",
    "train['Trap'] = train['Trap'].astype('category')\n",
    "train['AddressNumberAndStreet'] = train['AddressNumberAndStreet'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../weather-nmo.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to create one line per date:\n",
    "station1 = weather[weather['Station']==1]\n",
    "station2 = weather[weather['Station']==2]\n",
    "station1 = station1.drop('Station', axis=1)\n",
    "station2 = station2.drop('Station', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station1.columns = ['Date', 'st1_Tmax', 'st1_Tmin', 'st1_Tavg', 'st1_DewPoint', 'st1_WetBulb', 'st1_CodeSum',\n",
    "       'st1_SnowFall', 'st1_PrecipTotal', 'st1_StnPressure', 'st1_SeaLevel', 'st1_ResultSpeed',\n",
    "       'st1_ResultDir', 'st1_AvgSpeed', 'st1_Lat', 'st1_Long']\n",
    "station2.columns = ['Date', 'st2_Tmax', 'st2_Tmin', 'st2_Tavg', 'st2_DewPoint', 'st2_WetBulb', 'st2_CodeSum',\n",
    "       'st2_SnowFall', 'st2_PrecipTotal', 'st2_StnPressure', 'st2_SeaLevel', 'st2_ResultSpeed',\n",
    "       'st2_ResultDir', 'st2_AvgSpeed', 'st2_Lat', 'st2_Long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = pd.merge(station1, station2, on='Date')\n",
    "weather[\"Date\"] = pd.to_datetime(weather[\"Date\"], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature engineer us some over time weather data\n",
    "weather = weather.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather['precip_avg'] = (weather['st1_PrecipTotal'] + weather['st2_PrecipTotal'])/2\n",
    "weather['2wk_precip'] = weather['precip_avg'].rolling(14, min_periods=1).sum()\n",
    "weather['4wk_precip'] = weather['precip_avg'].rolling(28, min_periods=1).sum()\n",
    "weather['90day_precip'] = weather['precip_avg'].rolling(90, min_periods=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather['temp_avg'] = (weather['st1_Tavg'] + weather['st2_Tavg'])/2\n",
    "weather['2wk_tavg'] = weather['temp_avg'].rolling(14, min_periods=1).mean()\n",
    "weather['4wk_tavg'] = weather['temp_avg'].rolling(28, min_periods=1).mean()\n",
    "weather['90day_tavg'] = weather['temp_avg'].rolling(90, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather['tempmin_avg'] = (weather['st1_Tmin'] + weather['st2_Tmin'])/2\n",
    "weather['2wk_mintemp'] = weather['tempmin_avg'].rolling(14, min_periods=1).min()\n",
    "weather['4wk_mintemp'] = weather['tempmin_avg'].rolling(28, min_periods=1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather['dew_avg'] = (weather['st1_DewPoint'] + weather['st2_DewPoint'])/2\n",
    "weather['2wk_dew'] = weather['dew_avg'].rolling(14, min_periods=1).mean()\n",
    "weather['4wk_dew'] = weather['dew_avg'].rolling(28, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = weather.reset_index()\n",
    "train = pd.merge(train, weather, how='left', on='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.get_dummies(train, columns=['Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df['Month'] = final_df['Date'].dt.month\n",
    "final_df[\"Day\"] = final_df['Date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#our two origins (the locations with the most WNV activity) are Chicago O'Hare and Doty Ave.\n",
    "#the following values are their latitudes and longitudes\n",
    "ohare_lon = -87.890615\n",
    "ohare_lat = 41.974689\n",
    "doty_lon =-87.599862\n",
    "doty_lat=41.673408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat = train.Latitude\n",
    "lon = train.Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#haversine takes two lat and longs and creates a distance, from the mean, in miles\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    mi = 3956   * c #Radius of earth in miles. Use 6367 for kilometers\n",
    "    return mi, dlon, dlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply haversine function to training dataset, creating a column called 'dist_from_ohare_MI'\n",
    "final_df['dist_from_ohare_MI'] = [haversine(y, x, ohare_lon, ohare_lat)[0] for y, x in zip(lon, lat)]\n",
    "#apply haversine function to training dataset, creating a column called 'dist_from_doty_MI'\n",
    "final_df['dist_from_doty_MI'] = [haversine(y, x, doty_lon, doty_lat)[0] for y, x in zip(lon, lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Address', 'Block', 'Street', 'Trap', 'AddressNumberAndStreet',\n",
       "       'Latitude', 'Longitude', 'AddressAccuracy', 'NumMosquitos',\n",
       "       'WnvPresent', 'st1_Tmax', 'st1_Tmin', 'st1_Tavg', 'st1_DewPoint',\n",
       "       'st1_WetBulb', 'st1_CodeSum', 'st1_SnowFall', 'st1_PrecipTotal',\n",
       "       'st1_StnPressure', 'st1_SeaLevel', 'st1_ResultSpeed', 'st1_ResultDir',\n",
       "       'st1_AvgSpeed', 'st1_Lat', 'st1_Long', 'st2_Tmax', 'st2_Tmin',\n",
       "       'st2_Tavg', 'st2_DewPoint', 'st2_WetBulb', 'st2_CodeSum',\n",
       "       'st2_SnowFall', 'st2_PrecipTotal', 'st2_StnPressure', 'st2_SeaLevel',\n",
       "       'st2_ResultSpeed', 'st2_ResultDir', 'st2_AvgSpeed', 'st2_Lat',\n",
       "       'st2_Long', 'precip_avg', '2wk_precip', '4wk_precip', '90day_precip',\n",
       "       'temp_avg', '2wk_tavg', '4wk_tavg', '90day_tavg', 'tempmin_avg',\n",
       "       '2wk_mintemp', '4wk_mintemp', 'dew_avg', '2wk_dew', '4wk_dew',\n",
       "       'Species_CULEX ERRATICUS', 'Species_CULEX PIPIENS',\n",
       "       'Species_CULEX PIPIENS/RESTUANS', 'Species_CULEX RESTUANS',\n",
       "       'Species_CULEX SALINARIUS', 'Species_CULEX TARSALIS',\n",
       "       'Species_CULEX TERRITANS', 'Month', 'Day', 'dist_from_ohare_MI',\n",
       "       'dist_from_doty_MI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df.dtypes\n",
    "test_features = final_df.drop(['Date', 'Address', 'Block', 'Street', 'Trap', 'AddressNumberAndStreet', 'NumMosquitos', 'WnvPresent', 'st1_CodeSum', 'st2_CodeSum'], 1)\n",
    "target = final_df.WnvPresent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = pd.DataFrame(scale.fit_transform(test_features), columns=test_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_features = final_df[['Latitude', 'Longitude', 'st1_Tmax', 'st1_Tmin', 'st1_Tavg', 'st1_DewPoint', 'st1_WetBulb', 'st1_SnowFall', 'st1_PrecipTotal', 'st1_StnPressure', 'st1_SeaLevel', 'st1_ResultSpeed', 'st1_ResultDir', 'st1_AvgSpeed', 'st2_Tmax', 'st2_Tmin', 'st2_Tavg', 'st2_DewPoint', 'st2_WetBulb', 'st2_SnowFall', 'st2_PrecipTotal', 'st2_StnPressure', 'st2_SeaLevel', 'st2_ResultSpeed', 'st2_ResultDir', 'st2_AvgSpeed', 'precip_avg', '2wk_precip', '4wk_precip', '90day_precip', 'temp_avg', '2wk_tavg', '4wk_tavg', '90day_tavg', 'tempmin_avg', '2wk_mintemp', '4wk_mintemp', 'dew_avg', '2wk_dew', '4wk_dew', 'Species_CULEX ERRATICUS', 'Species_CULEX PIPIENS', 'Species_CULEX PIPIENS/RESTUANS', 'Species_CULEX RESTUANS', 'Species_CULEX SALINARIUS', 'Species_CULEX TARSALIS', 'Species_CULEX TERRITANS', 'Month', 'Day', 'dist_from_ohare_MI', 'dist_from_doty_MI']]\n",
    "#target = final_df.WnvPresent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues: [ 16.02754323   6.62668508   3.98100632   3.26949224   2.73225848\n",
      "   2.37053243   1.79455563   1.62457177   1.56254917   1.28893934]\n",
      "\n",
      "explained variance pct: [ 0.31423564  0.12992263  0.07805152  0.06410159  0.0535686   0.0464766\n",
      "  0.03518402  0.03185132  0.0306353   0.02527091]\n",
      "\n",
      "0.809298130939\n"
     ]
    }
   ],
   "source": [
    "## Return the eigenvalues and the explained variance.\n",
    "## NOTE: In sklearn, explained_variance_ returns the eigenvalues\n",
    "## while explained_variance_ratio_ gives us the actual explained variance.\n",
    "\n",
    "exp_var_eigenvals = pca.explained_variance_\n",
    "exp_var_pct = pca.explained_variance_ratio_\n",
    "\n",
    "print('eigenvalues:', exp_var_eigenvals)\n",
    "print('')\n",
    "print('explained variance pct:', exp_var_pct)\n",
    "print('')\n",
    "print(exp_var_pct.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcs_data = pca.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For testing different PCAs\n",
    "#pcs_data = pd.DataFrame(pcs_data).iloc[:,:18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(test_features, target, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(pcs_data, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_auc_score\n",
    "\n",
    "def eval_sklearn_model(y_true, predictions, model=None, X=None):\n",
    "    \"\"\"This function takes the true values for y and the predictions made by the model and prints out the confusion matrix along with Accuracy, Precision, and, if model and X provided, Roc_Auc Scores.\"\"\"\n",
    "    cnf_matrix = confusion_matrix(y_true, predictions)\n",
    "\n",
    "    print('True Negative: ', cnf_matrix[0, 0], '| False Positive: ', cnf_matrix[0, 1])\n",
    "    print('False Negative: ', cnf_matrix[1, 0], '| True Positive: ', cnf_matrix[1, 1], '\\n')\n",
    "\n",
    "    sensitivity = cnf_matrix[1, 1]/ (cnf_matrix[1, 0] + cnf_matrix[1, 1])\n",
    "    specificity = cnf_matrix[0, 0]/ (cnf_matrix[0, 1] + cnf_matrix[0, 0])\n",
    "\n",
    "    print('Sensitivity (TP/ TP + FN): ', sensitivity)\n",
    "    print('Specificity (TN/ TN + FP): ', specificity, '\\n')\n",
    "\n",
    "    print('Accuracy: ', accuracy_score(y_true, predictions, normalize=True))\n",
    "    print('Precision: ', precision_score(y_true, predictions))\n",
    "    if model != None:\n",
    "        print('Roc-Auc: ', roc_auc_score(y_true, [x[1] for x in model.predict_proba(X)]))\n",
    "    else:\n",
    "        pass\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0.35, learning_rate=0.02, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=18.101298701298703, seed=0, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(scale_pos_weight=(6969/385), objective='binary:logistic', gamma=0.35, learning_rate=0.02, max_depth=3, n_estimators=200)\n",
    "# make sure to pick the correct objective for the problem\n",
    "# scale_pos_weight is supposed to help with unbalanced classes; it recommended number of negative cases divided by positive\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative:  2193 | False Positive:  793\n",
      "False Negative:  44 | True Positive:  122 \n",
      "\n",
      "Sensitivity (TP/ TP + FN):  0.734939759036\n",
      "Specificity (TN/ TN + FP):  0.734427327528 \n",
      "\n",
      "Accuracy:  0.734454314721\n",
      "Precision:  0.133333333333\n",
      "Roc-Auc:  0.825690572067\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = xgb.predict(X_test)\n",
    "eval_sklearn_model(y_test, test_predictions, model=xgb, X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEFORE PCA\n",
    "True Negative:  2125 | False Positive:  861\n",
    "False Negative:  29 | True Positive:  137 \n",
    "\n",
    "Sensitivity (TP/ TP + FN):  0.825301204819\n",
    "Specificity (TN/ TN + FP):  0.71165438714 \n",
    "\n",
    "Accuracy:  0.717639593909\n",
    "Precision:  0.137274549098\n",
    "Roc-Auc:  0.842136799038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Random Forest TEST SCORE:\n",
      "\n",
      "True Negative:  2423 | False Positive:  563\n",
      "False Negative:  52 | True Positive:  114 \n",
      "\n",
      "Sensitivity (TP/ TP + FN):  0.686746987952\n",
      "Specificity (TN/ TN + FP):  0.811453449431 \n",
      "\n",
      "Accuracy:  0.804885786802\n",
      "Precision:  0.168389955687\n",
      "Roc-Auc:  0.839361800854\n",
      "\n",
      "\n",
      "Wall time: 361 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "etc = ExtraTreesClassifier(class_weight='balanced', max_features='sqrt', min_samples_leaf=5, n_estimators=100, n_jobs=-1)\n",
    "etc.fit(X_train, y_train)\n",
    "test_predictions = etc.predict(X_test)\n",
    "print('Extra Random Forest TEST SCORE:\\n')\n",
    "eval_sklearn_model(y_test, test_predictions,model=etc,X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest TEST SCORE:\n",
      "\n",
      "True Negative:  2731 | False Positive:  255\n",
      "False Negative:  90 | True Positive:  76 \n",
      "\n",
      "Sensitivity (TP/ TP + FN):  0.457831325301\n",
      "Specificity (TN/ TN + FP):  0.914601473543 \n",
      "\n",
      "Accuracy:  0.890545685279\n",
      "Precision:  0.229607250755\n",
      "Roc-Auc:  0.839361800854\n",
      "\n",
      "\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier(class_weight='balanced', max_features='sqrt', min_samples_leaf=5, n_estimators=1000, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)\n",
    "test_predictions = rfc.predict(X_test)\n",
    "print('Random Forest TEST SCORE:\\n')\n",
    "eval_sklearn_model(y_test, test_predictions,model=etc,X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Models Run:  120\n",
      "Decision Tree Classifier Score: 0.798 \n",
      "\n",
      "Elapsed Time: 1.39e+02  seconds \n",
      "\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='gini', max_depth=None, max_features='sqrt',\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=6,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=1000, n_jobs=-1, oob_score=False,\n",
      "           random_state=None, verbose=0, warm_start=False) \n",
      "\n",
      "Best Hyperparameters we tested for \n",
      " {'params': [('max_features', 'sqrt'), ('min_samples_leaf', 6), ('n_estimators', 1000)], 'score': 0.79040803737396559}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Pick which estimators you want to test (example is for random forest)\n",
    "param_grid = dict(n_estimators = [100, 1000],\n",
    "                 max_features = [10, 20, 30, 'sqrt'],\n",
    "                 min_samples_leaf = [2, 3, 4, 5, 6],\n",
    "                 )\n",
    "# How many cross validation folds do you want?\n",
    "cross_val=3\n",
    "\n",
    "# Switch out the model here that you would like to test\n",
    "model = ExtraTreesClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=cross_val, scoring='roc_auc', verbose=1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_results = {'params': list(grid.best_params_.items()), 'score': grid.best_score_}\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best_model = best_model.fit(X_train, y_train)\n",
    "\n",
    "score = best_model.score(X_test, y_test)\n",
    "\n",
    "print('Number of Models Run: ', np.prod([len(param_grid[i]) for i in param_grid]) * cross_val)\n",
    "print(\"{} Score: {:0.3}\".format('Decision Tree Classifier', score.mean().round(3)), '\\n')\n",
    "print('Elapsed Time: {:0.3}'.format( time.time() - start_time), ' seconds', '\\n')\n",
    "print(grid.best_estimator_, '\\n')\n",
    "print('Best Hyperparameters we tested for', '\\n', best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search TEST SCORE:\n",
      "\n",
      "True Negative:  2398 | False Positive:  588\n",
      "False Negative:  49 | True Positive:  117 \n",
      "\n",
      "Sensitivity (TP/ TP + FN):  0.704819277108\n",
      "Specificity (TN/ TN + FP):  0.803081044876 \n",
      "\n",
      "Accuracy:  0.797906091371\n",
      "Precision:  0.165957446809\n",
      "Roc-Auc:  0.842486826072\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your best model from the grid is already fit and saved as best_model\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print('Grid Search TEST SCORE:\\n')\n",
    "# function created above should be run before this cell\n",
    "eval_sklearn_model(y_test, test_predictions, model=best_model, X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Models Run:  120\n",
      "Decision Tree Classifier Score: 0.899 \n",
      "\n",
      "Elapsed Time: 4.21e+02  seconds \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features=20,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=6,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1000, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False) \n",
      "\n",
      "Best Hyperparameters we tested for \n",
      " {'params': [('max_features', 20), ('min_samples_leaf', 6), ('n_estimators', 1000)], 'score': 0.80202909231930231}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Pick which estimators you want to test (example is for random forest)\n",
    "param_grid = dict(n_estimators = [1000, 2000],\n",
    "                 max_features = [10, 20, 30, 'sqrt'],\n",
    "                 min_samples_leaf = [2, 3, 4, 5, 6],\n",
    "                 )\n",
    "# How many cross validation folds do you want?\n",
    "cross_val=3\n",
    "\n",
    "# Switch out the model here that you would like to test\n",
    "model = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=cross_val, scoring='roc_auc', verbose=1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_results = {'params': list(grid.best_params_.items()), 'score': grid.best_score_}\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best_model = best_model.fit(X_train, y_train)\n",
    "\n",
    "score = best_model.score(X_test, y_test)\n",
    "\n",
    "print('Number of Models Run: ', np.prod([len(param_grid[i]) for i in param_grid]) * cross_val)\n",
    "print(\"{} Score: {:0.3}\".format('Decision Tree Classifier', score.mean().round(3)), '\\n')\n",
    "print('Elapsed Time: {:0.3}'.format( time.time() - start_time), ' seconds', '\\n')\n",
    "print(grid.best_estimator_, '\\n')\n",
    "print('Best Hyperparameters we tested for', '\\n', best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search TEST SCORE:\n",
      "\n",
      "True Negative:  2762 | False Positive:  224\n",
      "False Negative:  95 | True Positive:  71 \n",
      "\n",
      "Sensitivity (TP/ TP + FN):  0.427710843373\n",
      "Specificity (TN/ TN + FP):  0.924983255191 \n",
      "\n",
      "Accuracy:  0.898794416244\n",
      "Precision:  0.240677966102\n",
      "Roc-Auc:  0.841158337301\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your best model from the grid is already fit and saved as best_model\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print('Grid Search TEST SCORE:\\n')\n",
    "# function created above should be run before this cell\n",
    "eval_sklearn_model(y_test, test_predictions, model=best_model, X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>import</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Day</td>\n",
       "      <td>0.123397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>0.100915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>dist_from_ohare_MI</td>\n",
       "      <td>0.086271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4wk_dew</td>\n",
       "      <td>0.081232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>dist_from_doty_MI</td>\n",
       "      <td>0.074891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>0.074716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>90day_tavg</td>\n",
       "      <td>0.054776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Month</td>\n",
       "      <td>0.042208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4wk_tavg</td>\n",
       "      <td>0.034593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4wk_mintemp</td>\n",
       "      <td>0.025118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>st2_ResultSpeed</td>\n",
       "      <td>0.020467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2wk_tavg</td>\n",
       "      <td>0.018703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Species_CULEX PIPIENS/RESTUANS</td>\n",
       "      <td>0.018030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2wk_dew</td>\n",
       "      <td>0.017803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Species_CULEX RESTUANS</td>\n",
       "      <td>0.016864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>90day_precip</td>\n",
       "      <td>0.016047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Species_CULEX PIPIENS</td>\n",
       "      <td>0.015010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>st1_ResultSpeed</td>\n",
       "      <td>0.013807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>st1_AvgSpeed</td>\n",
       "      <td>0.012220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>st2_AvgSpeed</td>\n",
       "      <td>0.010567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>st1_Tmax</td>\n",
       "      <td>0.009464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>temp_avg</td>\n",
       "      <td>0.009389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2wk_mintemp</td>\n",
       "      <td>0.009080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2wk_precip</td>\n",
       "      <td>0.008707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4wk_precip</td>\n",
       "      <td>0.008594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>st2_Tmin</td>\n",
       "      <td>0.007950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>st2_StnPressure</td>\n",
       "      <td>0.006917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>st2_Tmax</td>\n",
       "      <td>0.006778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>st1_Tavg</td>\n",
       "      <td>0.006586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>st2_ResultDir</td>\n",
       "      <td>0.006364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>st1_StnPressure</td>\n",
       "      <td>0.006353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>st1_ResultDir</td>\n",
       "      <td>0.005691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>st2_SeaLevel</td>\n",
       "      <td>0.004887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>st2_Tavg</td>\n",
       "      <td>0.004743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st1_Tmin</td>\n",
       "      <td>0.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tempmin_avg</td>\n",
       "      <td>0.004523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>st1_SeaLevel</td>\n",
       "      <td>0.004506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>precip_avg</td>\n",
       "      <td>0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>st1_PrecipTotal</td>\n",
       "      <td>0.003565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>st2_WetBulb</td>\n",
       "      <td>0.003539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dew_avg</td>\n",
       "      <td>0.003244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>st1_WetBulb</td>\n",
       "      <td>0.002983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>st1_DewPoint</td>\n",
       "      <td>0.002921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>st2_PrecipTotal</td>\n",
       "      <td>0.002744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>st2_DewPoint</td>\n",
       "      <td>0.002622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Species_CULEX TERRITANS</td>\n",
       "      <td>0.001804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Species_CULEX ERRATICUS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Species_CULEX SALINARIUS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Species_CULEX TARSALIS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>st2_SnowFall</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>st1_SnowFall</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature    import\n",
       "48                             Day  0.123397\n",
       "1                        Longitude  0.100915\n",
       "49              dist_from_ohare_MI  0.086271\n",
       "39                         4wk_dew  0.081232\n",
       "50               dist_from_doty_MI  0.074891\n",
       "0                         Latitude  0.074716\n",
       "33                      90day_tavg  0.054776\n",
       "47                           Month  0.042208\n",
       "32                        4wk_tavg  0.034593\n",
       "36                     4wk_mintemp  0.025118\n",
       "23                 st2_ResultSpeed  0.020467\n",
       "31                        2wk_tavg  0.018703\n",
       "42  Species_CULEX PIPIENS/RESTUANS  0.018030\n",
       "38                         2wk_dew  0.017803\n",
       "43          Species_CULEX RESTUANS  0.016864\n",
       "29                    90day_precip  0.016047\n",
       "41           Species_CULEX PIPIENS  0.015010\n",
       "11                 st1_ResultSpeed  0.013807\n",
       "13                    st1_AvgSpeed  0.012220\n",
       "25                    st2_AvgSpeed  0.010567\n",
       "2                         st1_Tmax  0.009464\n",
       "30                        temp_avg  0.009389\n",
       "35                     2wk_mintemp  0.009080\n",
       "27                      2wk_precip  0.008707\n",
       "28                      4wk_precip  0.008594\n",
       "15                        st2_Tmin  0.007950\n",
       "21                 st2_StnPressure  0.006917\n",
       "14                        st2_Tmax  0.006778\n",
       "4                         st1_Tavg  0.006586\n",
       "24                   st2_ResultDir  0.006364\n",
       "9                  st1_StnPressure  0.006353\n",
       "12                   st1_ResultDir  0.005691\n",
       "22                    st2_SeaLevel  0.004887\n",
       "16                        st2_Tavg  0.004743\n",
       "3                         st1_Tmin  0.004622\n",
       "34                     tempmin_avg  0.004523\n",
       "10                    st1_SeaLevel  0.004506\n",
       "26                      precip_avg  0.003790\n",
       "8                  st1_PrecipTotal  0.003565\n",
       "18                     st2_WetBulb  0.003539\n",
       "37                         dew_avg  0.003244\n",
       "6                      st1_WetBulb  0.002983\n",
       "5                     st1_DewPoint  0.002921\n",
       "20                 st2_PrecipTotal  0.002744\n",
       "17                    st2_DewPoint  0.002622\n",
       "46         Species_CULEX TERRITANS  0.001804\n",
       "40         Species_CULEX ERRATICUS  0.000000\n",
       "44        Species_CULEX SALINARIUS  0.000000\n",
       "45          Species_CULEX TARSALIS  0.000000\n",
       "19                    st2_SnowFall  0.000000\n",
       "7                     st1_SnowFall  0.000000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_import = best_model.feature_importances_\n",
    "features = X_train.columns\n",
    "pd.DataFrame(columns=['feature', 'import'], data=list(zip(features, feature_import))).sort_values('import',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search TEST SCORE:\n",
      "\n",
      "True Negative:  2763 | False Positive:  223\n",
      "False Negative:  94 | True Positive:  72 \n",
      "\n",
      "Sensitivity (TP/ TP + FN):  0.433734939759\n",
      "Specificity (TN/ TN + FP):  0.925318151373 \n",
      "\n",
      "Accuracy:  0.89942893401\n",
      "Precision:  0.24406779661\n",
      "Roc-Auc:  0.841764580089\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(class_weight='balanced', n_jobs=-1, max_features=20, min_samples_leaf=6, n_estimators=1000, random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "print('Grid Search TEST SCORE:\\n')\n",
    "# function created above should be run before this cell\n",
    "eval_sklearn_model(y_test, test_predictions, model=model, X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', penalty = 'l1', random_state = 42)\n",
    "svm = SVC(class_weight='balanced', C=1.0, gamma='auto', kernel='poly', probability = True, random_state = 42)\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1, max_features=8, min_samples_leaf=6, n_estimators=1000, random_state = 42)\n",
    "etc = ExtraTreesClassifier(class_weight='balanced', n_jobs=-1, max_features='sqrt', min_samples_leaf=6, n_estimators=1000, random_state = 42)\n",
    "xgb = XGBClassifier(scale_pos_weight=(6969/385), objective='binary:logistic', gamma=0.35, learning_rate=0.02, max_depth=3, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voter = VotingClassifier(estimators = [('lr', lr),\n",
    "                                       ('extra trees', etc),\n",
    "                                       ('random forest', rf),\n",
    "                                       ('svm', svm),\n",
    "                                       ('xgb', xgb),\n",
    "                                       ],\n",
    "                        voting = 'soft', weights = [1, 2, 1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.4 s, sys: 552 ms, total: 59 s\n",
      "Wall time: 29.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)), ('extra tr...reg_lambda=1,\n",
       "       scale_pos_weight=18.101298701298703, seed=0, silent=True,\n",
       "       subsample=1))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft',\n",
       "         weights=[1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "voter.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voter TEST SCORES:\n",
      "\n",
      "True Negative:  2628 | False Positive:  358\n",
      "False Negative:  69 | True Positive:  97 \n",
      "\n",
      "Sensitivity (TP/ TP + FN):  0.584337349398\n",
      "Specificity (TN/ TN + FP):  0.880107166778 \n",
      "\n",
      "Accuracy:  0.864530456853\n",
      "Precision:  0.213186813187\n",
      "Roc-Auc:  0.834230424713\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = voter.predict(X_test)\n",
    "print('Voter TEST SCORES:\\n')\n",
    "eval_sklearn_model(y_test, predictions, model=voter, X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up test data and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../assets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[\"Date\"] = pd.to_datetime(test[\"Date\"], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[\"Date\"] = pd.to_datetime(test[\"Date\"], infer_datetime_format=True)\n",
    "test['Address'] = test['Address'].astype('category')\n",
    "test['Species'] = test['Species'].astype('category')\n",
    "test['Street'] = test['Street'].astype('category')\n",
    "test['Trap'] = test['Trap'].astype('category')\n",
    "test['AddressNumberAndStreet'] = test['AddressNumberAndStreet'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.merge(test, weather, how='left', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.get_dummies(test, columns=['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Month'] = test['Date'].dt.month\n",
    "test[\"Day\"] = test['Date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat = test.Latitude\n",
    "lon = test.Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply haversine function to training dataset, creating a column called 'dist_from_ohare_MI'\n",
    "test['dist_from_ohare_MI'] = [haversine(y, x, ohare_lon, ohare_lat)[0] for y, x in zip(lon, lat)]\n",
    "#apply haversine function to training dataset, creating a column called 'dist_from_doty_MI'\n",
    "test['dist_from_doty_MI'] = [haversine(y, x, doty_lon, doty_lat)[0] for y, x in zip(lon, lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make match above\n",
    "# Non-PCA\n",
    "#pred_features = test[['Latitude', 'Longitude', 'st1_Tmax', 'st1_Tmin', 'st1_Tavg', 'st1_DewPoint', 'st1_WetBulb', 'st1_SnowFall', 'st1_PrecipTotal', 'st1_StnPressure', 'st1_SeaLevel', 'st1_ResultSpeed', 'st1_ResultDir', 'st1_AvgSpeed', 'st2_Tmax', 'st2_Tmin', 'st2_Tavg', 'st2_DewPoint', 'st2_WetBulb', 'st2_SnowFall', 'st2_PrecipTotal', 'st2_StnPressure', 'st2_SeaLevel', 'st2_ResultSpeed', 'st2_ResultDir', 'st2_AvgSpeed', 'precip_avg', '2wk_precip', '4wk_precip', '90day_precip', 'temp_avg', '2wk_tavg', '4wk_tavg', '90day_tavg', 'tempmin_avg', '2wk_mintemp', '4wk_mintemp', 'dew_avg', '2wk_dew', '4wk_dew', 'Species_CULEX ERRATICUS', 'Species_CULEX PIPIENS', 'Species_CULEX PIPIENS/RESTUANS', 'Species_CULEX RESTUANS', 'Species_CULEX SALINARIUS', 'Species_CULEX TARSALIS', 'Species_CULEX TERRITANS', 'Month', 'Day', 'dist_from_ohare_MI', 'dist_from_doty_MI']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IF PCA\n",
    "pred_features = test.drop(['Id', 'Species_UNSPECIFIED CULEX', 'Date', 'Address', 'Block', 'Street', 'Trap', 'AddressNumberAndStreet', 'st1_CodeSum', 'st2_CodeSum'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_features = pd.DataFrame(scale.fit_transform(pred_features), columns=pred_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10506, 10)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 56)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA ONLY\n",
    "pred_features = pca.transform(pred_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatever model you decided on:\n",
    "#predictions = xgb.predict(pred_features)\n",
    "predictions = voter.predict_proba(pred_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['Id', 'WnvPresent'], data=list(zip(test.Id, predictions)))\n",
    "submission = submission.set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For predict_proba only\n",
    "submission.WnvPresent = submission.WnvPresent.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run with predict_proba\n",
    "#submission['WnvPresent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
